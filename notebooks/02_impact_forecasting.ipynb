{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Impact Modeling & Forecasting\n",
    "\n",
    "## Objectives\n",
    "- Build Association Matrix.\n",
    "- Build Event-Augmented Trend Model.\n",
    "- Forecast 2025-2027.\n",
    "- Generate Uncertainty Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\data\\raw\\ethiopia_fi_unified_data.xlsx...\n",
      "Enriching data with new records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\src\\data_loader.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_unified = pd.concat([df_unified, pd.DataFrame([new_obs])], ignore_index=True)\n",
      "c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\src\\data_loader.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_unified = pd.concat([df_unified, pd.DataFrame([new_obs])], ignore_index=True)\n",
      "c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\src\\data_loader.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_unified = pd.concat([df_unified, pd.DataFrame([new_event])], ignore_index=True)\n",
      "c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\src\\data_loader.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_unified = pd.concat([df_unified, pd.DataFrame([new_event])], ignore_index=True)\n",
      "c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\src\\data_loader.py:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_unified = pd.concat([df_unified, pd.DataFrame([new_event])], ignore_index=True)\n",
      "c:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\src\\data_loader.py:68: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_impact = pd.concat([df_impact, pd.DataFrame([new_impact])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "\n",
    "# Ensure reports/figures exists\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join('../src')))\n",
    "from data_loader import load_raw_data, enrich_data, process_data\n",
    "\n",
    "# Load Data\n",
    "df_u, df_i = load_raw_data()\n",
    "df_u, df_i = enrich_data(df_u, df_i)\n",
    "observations, events_enriched, raw_impacts = process_data(df_u, df_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Association Matrix\n",
    "Mapping Events to Key Indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'related_indicator' missing or empty in events.\n"
     ]
    }
   ],
   "source": [
    "# Filter events that impact ACC_OWNERSHIP or USG_DIGITAL_PAYMENT\n",
    "target_indicators = ['ACC_OWNERSHIP', 'USG_DIGITAL_PAYMENT']\n",
    "\n",
    "# Check if related_indicator column exists and has values\n",
    "if 'related_indicator' in events_enriched.columns:\n",
    "    relevant_events = events_enriched[events_enriched['related_indicator'].isin(target_indicators)].copy()\n",
    "    \n",
    "    # Create a simplistic matrix (Event x Indicator)\n",
    "    # We want to show which event affects which indicator\n",
    "    association = pd.crosstab(relevant_events['original_text_evt'], relevant_events['related_indicator'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(association, annot=True, cmap='Blues', cbar=False)\n",
    "    plt.title('Event-Indicator Association Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/association_matrix.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Column 'related_indicator' missing or empty in events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Event-Augmented Trend Model\n",
    "Baseline Trend + Impact Boosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'impact_boost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'impact_boost'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ts_data\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Run for Account Ownership\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m forecast_df = \u001b[43mforecast_with_impacts\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mACC_OWNERSHIP\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m forecast_df.tail()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mforecast_with_impacts\u001b[39m\u001b[34m(indicator_code, start_year, end_year)\u001b[39m\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# Apply boost to all years >= evt_year\u001b[39;00m\n\u001b[32m     43\u001b[39m         ts_data.loc[ts_data[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] >= evt_year, \u001b[33m'\u001b[39m\u001b[33mimpact_boost\u001b[39m\u001b[33m'\u001b[39m] += magnitude\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m ts_data[\u001b[33m'\u001b[39m\u001b[33mforecast\u001b[39m\u001b[33m'\u001b[39m] = ts_data[\u001b[33m'\u001b[39m\u001b[33mbaseline_trend\u001b[39m\u001b[33m'\u001b[39m] + \u001b[43mts_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimpact_boost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ts_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Downloads\\KAIM\\KAIM WEEK 10\\Forecasting-Digital-Finance-Ethiopia\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'impact_boost'"
     ]
    }
   ],
   "source": [
    "def forecast_with_impacts(indicator_code, start_year=2011, end_year=2027):\n",
    "    # 1. Get Baseline Data\n",
    "    data = observations[observations['indicator_code'] == indicator_code].sort_values('observation_date')\n",
    "    data['year'] = data['observation_date'].dt.year\n",
    "    \n",
    "    # Create a full year range dataframe\n",
    "    years = pd.DataFrame({'year': range(start_year, end_year + 1)})\n",
    "    ts_data = pd.merge(years, data[['year', 'value_numeric']], on='year', how='left')\n",
    "    \n",
    "    # Interpolate missing values for baseline trend (Linear)\n",
    "    ts_data['baseline'] = ts_data['value_numeric'].interpolate(method='linear')\n",
    "    # Forward fill for the future if last point is earlier, or extrapolate\n",
    "    # Simple extrapolation: last known growth rate or just linear projection\n",
    "    # For simplicity, let's use a linear regression on available points to project baseline\n",
    "    valid_data = ts_data.dropna(subset=['value_numeric'])\n",
    "    if len(valid_data) > 1:\n",
    "        z = np.polyfit(valid_data['year'], valid_data['value_numeric'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ts_data['baseline_trend'] = p(ts_data['year'])\n",
    "    else:\n",
    "        ts_data['baseline_trend'] = ts_data['baseline'] # Fallback\n",
    "\n",
    "    # 2. Add Impacts\n",
    "    # Find events related to this indicator\n",
    "    if 'related_indicator' in events_enriched.columns:\n",
    "        related_evts = events_enriched[events_enriched['related_indicator'] == indicator_code]\n",
    "        \n",
    "        ts_data['impact_boost'] = 0.0\n",
    "        for _, evt in related_evts.iterrows():\n",
    "            # Assuming impact_magnitude is a percentage point boost\n",
    "            # Distributed over 'lag_months' or immediate?\n",
    "            # Let's simplify: Add magnitude cumulatively starting from event year\n",
    "            evt_year = evt['observation_date_evt'].year\n",
    "            magnitude = evt['impact_magnitude_imp'] if pd.notna(evt['impact_magnitude_imp']) else 0\n",
    "            \n",
    "            # Check direction\n",
    "            if evt['impact_direction_imp'] == 'decrease':\n",
    "                magnitude = -abs(magnitude)\n",
    "            else:\n",
    "                magnitude = abs(magnitude)\n",
    "                \n",
    "            # Apply boost to all years >= evt_year\n",
    "            ts_data.loc[ts_data['year'] >= evt_year, 'impact_boost'] += magnitude\n",
    "\n",
    "    ts_data['forecast'] = ts_data['baseline_trend'] + ts_data['impact_boost']\n",
    "    \n",
    "    return ts_data\n",
    "\n",
    "# Run for Account Ownership\n",
    "forecast_df = forecast_with_impacts('ACC_OWNERSHIP')\n",
    "forecast_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forecast & Uncertainty Plots\n",
    "Projecting 2025-2027 with Confidence Intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (126236280.py, line 25)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplt.savefig(f'../reports/figures/{filename}'\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "def plot_forecast(df, title, filename):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Historical Data\n",
    "    plt.plot(df['year'], df['value_numeric'], 'ko', label='Historical Data')\n",
    "    \n",
    "    # Forecast Line\n",
    "    plt.plot(df['year'], df['forecast'], 'b-', label='Event-Augmented Forecast', linewidth=2)\n",
    "    \n",
    "    # Baseline (Counterfactual)\n",
    "    plt.plot(df['year'], df['baseline_trend'], 'g--', label='Baseline Only', alpha=0.5)\n",
    "    \n",
    "    # Confidence Intervals (Simple heuristic: +/- 5% growing over time)\n",
    "    uncertainty_grow = (df['year'] - df['year'].min()) * 0.5 # grows 0.5 pp per year\n",
    "    upper = df['forecast'] + 5 + uncertainty_grow\n",
    "    lower = df['forecast'] - 5 - uncertainty_grow\n",
    "    \n",
    "    plt.fill_between(df['year'], lower, upper, color='b', alpha=0.1, label='Confidence Interval (Optimistic/Pessimistic)')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'../reports/figures/{filename}')\n",
    "\n",
    "plot_forecast(forecast_df, 'Account Ownership Forecast (2025-2027)', 'forecast_acc_ownership.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for dashboard\n",
    "forecast_df.to_csv('../data/processed/forecast_results.csv', index=False)\n",
    "print(\"Forecast saved to data/processed/forecast_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
